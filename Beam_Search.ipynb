{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, latest_weights_file_path\n",
    "from train import get_model, get_ds, causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Max length of source sentence: 40\n",
      "Max length of target sentence: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename, map_location=torch.device(f'{device}'))\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    decoder_initial_input = torch.tensor([[sos_idx]], dtype=torch.long, device=device)\n",
    "\n",
    "    candidates = [(decoder_initial_input, 0)]  # (sequence, score)\n",
    "\n",
    "    while True:\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "            if candidate[0, -1].item() == eos_idx:\n",
    "                new_candidates.append((candidate, score))\n",
    "                continue\n",
    "\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            prob = model.project(out[:, -1])\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "        if all([cand[0, -1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, num_examples=5):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "            if encoder_input.size(0) != 1:\n",
    "                raise ValueError(\"Batch size must be 1 for validation.\")\n",
    "\n",
    "            greedy_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            beam_out = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            greedy_out_text = tokenizer_tgt.decode(greedy_out.detach().cpu().numpy())\n",
    "            beam_out_text = tokenizer_tgt.decode(beam_out.detach().cpu().numpy())\n",
    "\n",
    "            print_msg('-' * console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>20}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>20}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED GREEDY: ':>20}{greedy_out_text}\")\n",
    "            print_msg(f\"{f'PREDICTED BEAM: ':>20}{beam_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-' * console_width)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "            SOURCE: Then he took from his knapsack a bottle of wine, and drank some.\n",
      "\n",
      "            TARGET: ثم أخذ من حقيبة ظهره زجاجة من النبيذ واحتسى القليل.\n",
      "\n",
      "  PREDICTED GREEDY: ثم أخذ من حقيبة ظهره زجاجة من النبيذ القليل .\n",
      "    PREDICTED BEAM: ثم أخذ من حقيبة ظهره زجاجة من النبيذ القليل من الطعام . القليل من الطعام . الخمر . .\n",
      "--------------------------------------------------------------------------------\n",
      "            SOURCE: Members from the Wimbledon area have refused to consider such a plan.\n",
      "\n",
      "            TARGET: وقد رفض أعضاء من منطقة ويمبلدون النظر في مثل هذه الخطة.\n",
      "\n",
      "  PREDICTED GREEDY: وقد رفض أعضاء من منطقة ويمبلدون النظر في مثل هذه الخطة .\n",
      "    PREDICTED BEAM: وقد رفض أعضاء من منطقة ويمبلدون النظر في مثل هذه الخطة .\n",
      "--------------------------------------------------------------------------------\n",
      "            SOURCE: He gave no reason, but his motive was obvious enough.\n",
      "\n",
      "            TARGET: لم يذكر أي سبب لكن دافعه كان واضحًا بما فيه الكفاية.\n",
      "\n",
      "  PREDICTED GREEDY: لم يذكر أي سبب لكن كان واضحًا بما فيه الكفاية .\n",
      "    PREDICTED BEAM: لم يذكر أي سبب لكن كان واضحًا بما فيه الكفاية .\n",
      "--------------------------------------------------------------------------------\n",
      "            SOURCE: Homer agrees to stop after Ron Howard is injured during the chase.\n",
      "\n",
      "            TARGET: يوافق هومر على التوقف بعد إصابة رون هوارد أثناء المطاردة.\n",
      "\n",
      "  PREDICTED GREEDY: يوافق هومر على التوقف بعد إصابة رون هوارد أثناء المطاردة .\n",
      "    PREDICTED BEAM: يوافق هومر على التوقف بعد إصابة رون هوارد أثناء المطاردة . .\n",
      "--------------------------------------------------------------------------------\n",
      "            SOURCE: \"The primary species, \"\"Bathmochoffatia hapax\"\", was also named by Hahn and Hahn.\"\n",
      "\n",
      "            TARGET: الكائنات الحية الرئيسية \"باثموشوفياتيا هبيكسا\" تم تسميتها أيضًا من قبل هان وهان.\n",
      "\n",
      "  PREDICTED GREEDY: الكائنات الحية الرئيسية \" \" تم تسميتها أيضًا من قبل هان .\n",
      "    PREDICTED BEAM: الكائنات الحية الرئيسية \" \" تم تسميتها أيضًا من قبل هان .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, 50, device, print_msg=print, num_examples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
